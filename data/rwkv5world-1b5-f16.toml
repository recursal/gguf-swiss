manifest_version = 0

[tasks.model-card]
task = "add-model-card"
name = "RWKV5 World 1.5B"
author = "Recursal AI"
description = "Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages!"
license = "Apache-2.0"

[tasks.model-config]
task = "add-model-config"
architecture = "rwkv5"
embedding_length = 2048
block_count = 24
layer_norm_epsilon = 0.00001

# RWKV isn't context limited, but a context length is required
context_length = 1048576

# Required by llama.cpp, but we don't use them
feed_forward_length = 0
attention_head_count = 0

[tasks.tokenizer]
task = "convert-rwkv-tokenizer"
source = "vocab.txt"
token_count = 65536

[tasks.safetensors]
task = "convert-safetensors"
source = "model.safetensors"

[tasks.safetensors.tensors]
"token_embd.weight" = { source = "rwkv.embeddings.weight", type = "F16", dimensions = [2048, 65536] }

"blk.0.attn_norm.weight" = { source = "rwkv.blocks.0.ln1.weight", type = "F32", dimensions = [2048] }
"blk.0.attn_norm.bias" = { source = "rwkv.blocks.0.ln1.bias", type = "F32", dimensions = [2048] }

"blk.1.attn_norm.weight" = { source = "rwkv.blocks.1.ln1.weight", type = "F32", dimensions = [2048] }
"blk.1.attn_norm.bias" = { source = "rwkv.blocks.1.ln1.bias", type = "F32", dimensions = [2048] }

"blk.2.attn_norm.weight" = { source = "rwkv.blocks.2.ln1.weight", type = "F32", dimensions = [2048] }
"blk.2.attn_norm.bias" = { source = "rwkv.blocks.2.ln1.bias", type = "F32", dimensions = [2048] }

"blk.3.attn_norm.weight" = { source = "rwkv.blocks.3.ln1.weight", type = "F32", dimensions = [2048] }
"blk.3.attn_norm.bias" = { source = "rwkv.blocks.3.ln1.bias", type = "F32", dimensions = [2048] }

"blk.4.attn_norm.weight" = { source = "rwkv.blocks.4.ln1.weight", type = "F32", dimensions = [2048] }
"blk.4.attn_norm.bias" = { source = "rwkv.blocks.4.ln1.bias", type = "F32", dimensions = [2048] }

"blk.5.attn_norm.weight" = { source = "rwkv.blocks.5.ln1.weight", type = "F32", dimensions = [2048] }
"blk.5.attn_norm.bias" = { source = "rwkv.blocks.5.ln1.bias", type = "F32", dimensions = [2048] }

"blk.6.attn_norm.weight" = { source = "rwkv.blocks.6.ln1.weight", type = "F32", dimensions = [2048] }
"blk.6.attn_norm.bias" = { source = "rwkv.blocks.6.ln1.bias", type = "F32", dimensions = [2048] }

"blk.7.attn_norm.weight" = { source = "rwkv.blocks.7.ln1.weight", type = "F32", dimensions = [2048] }
"blk.7.attn_norm.bias" = { source = "rwkv.blocks.7.ln1.bias", type = "F32", dimensions = [2048] }

"blk.8.attn_norm.weight" = { source = "rwkv.blocks.8.ln1.weight", type = "F32", dimensions = [2048] }
"blk.8.attn_norm.bias" = { source = "rwkv.blocks.8.ln1.bias", type = "F32", dimensions = [2048] }

"blk.9.attn_norm.weight" = { source = "rwkv.blocks.9.ln1.weight", type = "F32", dimensions = [2048] }
"blk.9.attn_norm.bias" = { source = "rwkv.blocks.9.ln1.bias", type = "F32", dimensions = [2048] }

"blk.10.attn_norm.weight" = { source = "rwkv.blocks.10.ln1.weight", type = "F32", dimensions = [2048] }
"blk.10.attn_norm.bias" = { source = "rwkv.blocks.10.ln1.bias", type = "F32", dimensions = [2048] }

"blk.11.attn_norm.weight" = { source = "rwkv.blocks.11.ln1.weight", type = "F32", dimensions = [2048] }
"blk.11.attn_norm.bias" = { source = "rwkv.blocks.11.ln1.bias", type = "F32", dimensions = [2048] }

"blk.12.attn_norm.weight" = { source = "rwkv.blocks.12.ln1.weight", type = "F32", dimensions = [2048] }
"blk.12.attn_norm.bias" = { source = "rwkv.blocks.12.ln1.bias", type = "F32", dimensions = [2048] }

"blk.13.attn_norm.weight" = { source = "rwkv.blocks.13.ln1.weight", type = "F32", dimensions = [2048] }
"blk.13.attn_norm.bias" = { source = "rwkv.blocks.13.ln1.bias", type = "F32", dimensions = [2048] }

"blk.14.attn_norm.weight" = { source = "rwkv.blocks.14.ln1.weight", type = "F32", dimensions = [2048] }
"blk.14.attn_norm.bias" = { source = "rwkv.blocks.14.ln1.bias", type = "F32", dimensions = [2048] }

"blk.15.attn_norm.weight" = { source = "rwkv.blocks.15.ln1.weight", type = "F32", dimensions = [2048] }
"blk.15.attn_norm.bias" = { source = "rwkv.blocks.15.ln1.bias", type = "F32", dimensions = [2048] }

"blk.16.attn_norm.weight" = { source = "rwkv.blocks.16.ln1.weight", type = "F32", dimensions = [2048] }
"blk.16.attn_norm.bias" = { source = "rwkv.blocks.16.ln1.bias", type = "F32", dimensions = [2048] }

"blk.17.attn_norm.weight" = { source = "rwkv.blocks.17.ln1.weight", type = "F32", dimensions = [2048] }
"blk.17.attn_norm.bias" = { source = "rwkv.blocks.17.ln1.bias", type = "F32", dimensions = [2048] }

"blk.18.attn_norm.weight" = { source = "rwkv.blocks.18.ln1.weight", type = "F32", dimensions = [2048] }
"blk.18.attn_norm.bias" = { source = "rwkv.blocks.18.ln1.bias", type = "F32", dimensions = [2048] }

"blk.19.attn_norm.weight" = { source = "rwkv.blocks.19.ln1.weight", type = "F32", dimensions = [2048] }
"blk.19.attn_norm.bias" = { source = "rwkv.blocks.19.ln1.bias", type = "F32", dimensions = [2048] }

"blk.20.attn_norm.weight" = { source = "rwkv.blocks.20.ln1.weight", type = "F32", dimensions = [2048] }
"blk.20.attn_norm.bias" = { source = "rwkv.blocks.20.ln1.bias", type = "F32", dimensions = [2048] }

"blk.21.attn_norm.weight" = { source = "rwkv.blocks.21.ln1.weight", type = "F32", dimensions = [2048] }
"blk.21.attn_norm.bias" = { source = "rwkv.blocks.21.ln1.bias", type = "F32", dimensions = [2048] }

"blk.22.attn_norm.weight" = { source = "rwkv.blocks.22.ln1.weight", type = "F32", dimensions = [2048] }
"blk.22.attn_norm.bias" = { source = "rwkv.blocks.22.ln1.bias", type = "F32", dimensions = [2048] }

"blk.23.attn_norm.weight" = { source = "rwkv.blocks.23.ln1.weight", type = "F32", dimensions = [2048] }
"blk.23.attn_norm.bias" = { source = "rwkv.blocks.23.ln1.bias", type = "F32", dimensions = [2048] }

"output_norm.weight" = { source = "rwkv.ln_out.weight", type = "F32", dimensions = [2048] }
"output_norm.bias" = { source = "rwkv.ln_out.bias", type = "F32", dimensions = [2048] }

"output.weight" = { source = "head.weight", type = "F16", dimensions = [2048, 65536] }
