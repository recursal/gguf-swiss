manifest_version = 0

[tasks.model-card]
task = "add-model-card"
name = "RWKV5 World 1.5B"
author = "Recursal AI"
description = "Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages!"
license = "Apache-2.0"
architecture = "rwkv5"

[tasks.config]
task = "add-llama-config"
embedding_length = 2048
block_count = 24
layer_norm_epsilon = 0.00001

# RWKV isn't context limited, but a context length is required
context_length = 1048576

# These are set to placeholders for now
feed_forward_length = 1
attention_head_count = 1
attention_head_count_kv = 1

[tasks.tokenizer]
task = "convert-rwkv-tokenizer"
source = "vocab.txt"
token_count = 65536

[tasks.safetensors]
task = "convert-safetensors"
source = "model.safetensors"

[tasks.safetensors.tensors]
"token_embd.weight" = { source = "rwkv.embeddings.weight", type = "F16", dimensions = [2048, 65536] }
"blk.0.ln1.bias" = { source = "rwkv.blocks.0.ln1.bias", type = "F16", dimensions = [2048] }
