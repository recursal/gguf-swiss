manifest_version = 0

[tasks.model-card]
task = "add-model-card"
name = "RWKV5 World 1.5B"
author = "Recursal AI"
description = "Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages!"
license = "Apache-2.0"

[tasks.model-config]
task = "add-model-config"
architecture = "rwkv5"
embedding_length = 2048
block_count = 24
layer_norm_epsilon = 0.00001

# RWKV isn't context limited, but a context length is required
context_length = 1048576

# Required by llama.cpp, but we don't use them
feed_forward_length = 0
attention_head_count = 0

[tasks.tokenizer]
task = "convert-rwkv-tokenizer"
source = "vocab.txt"
token_count = 65536

[tasks.safetensors]
task = "convert-safetensors"
source = "model.safetensors"

[tasks.safetensors.tensors]
"token_embd.weight" = { source = "rwkv.embeddings.weight", type = "F16", dimensions = [2048, 65536] }
"tok_norm.weight" = { source = "rwkv.blocks.0.pre_ln.weight", type = "F32", dimensions = [2048] }
"tok_norm.bias" = { source = "rwkv.blocks.0.pre_ln.bias", type = "F32", dimensions = [2048] }
"output_norm.weight" = { source = "rwkv.ln_out.weight", type = "F32", dimensions = [2048] }
"output_norm.bias" = { source = "rwkv.ln_out.bias", type = "F32", dimensions = [2048] }
"output.weight" = { source = "head.weight", type = "F16", dimensions = [2048, 65536] }

[tasks.safetensors.tensors."$0..24"]
"blk.$.attn_norm.weight" = { source = "rwkv.blocks.$.ln1.weight", type = "F32", dimensions = [2048] }
"blk.$.attn_norm.bias" = { source = "rwkv.blocks.$.ln1.bias", type = "F32", dimensions = [2048] }
"blk.$.attn_norm_2.weight" = { source = "rwkv.blocks.$.ln2.weight", type = "F32", dimensions = [2048] }
"blk.$.attn_norm_2.bias" = { source = "rwkv.blocks.$.ln2.bias", type = "F32", dimensions = [2048] }
"blk.$.time_mix_k.weight" = { source = "rwkv.blocks.$.attention.time_mix_key", type = "F32", dimensions = [2048, 1, 1] }
"blk.$.time_mix_v.weight" = { source = "rwkv.blocks.$.attention.time_mix_value", type = "F32", dimensions = [2048, 1, 1] }
"blk.$.time_mix_r.weight" = { source = "rwkv.blocks.$.attention.time_mix_receptance", type = "F32", dimensions = [2048, 1, 1] }
"blk.$.time_mix_g.weight" = { source = "rwkv.blocks.$.attention.time_mix_gate", type = "F32", dimensions = [2048, 1, 1] }
